{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Analysis and Classification\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA) on the fake news dataset and evaluates different classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the parent directory to the path to import the models module\n",
    "sys.path.append('..')\n",
    "from models import load_data, LogisticModel, DistilBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = load_data('../data/fake_news_dataset.csv')\n",
    "df = data['full_data']\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{df['label'].value_counts()}\")\n",
    "print(f\"\\nClass distribution (percentage):\\n{df['label'].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='text_length', hue='label', bins=50, kde=True)\n",
    "plt.title('Text Length Distribution by Class')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Real News', 'Fake News'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for missing or null values\n",
    "print(\"Checking for missing values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "    \n",
    "# 2. Check label distribution and ensure binary classes\n",
    "print(\"\\nLabel distribution before cleaning:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nUnique labels:\", df['label'].unique())\n",
    "\n",
    "# 3. Check for duplicates\n",
    "duplicate_count = df.duplicated(['text', 'label']).sum()\n",
    "print(f\"\\nNumber of duplicate entries (same text and label): {duplicate_count}\")\n",
    "\n",
    "# Clean the data\n",
    "# Remove missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Ensure binary labels (0 for Real, 1 for Fake)\n",
    "if df['label'].dtype == object:\n",
    "    label_map = {'Real': 0, 'Fake': 1}\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['text', 'label'])\n",
    "\n",
    "# Show cleaned data stats\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(\"\\nLabel distribution after cleaning:\")\n",
    "print(df['label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Evaluate Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "logistic_model = LogisticModel(max_features=10000, preprocess=True)\n",
    "logistic_model.train(data['train']['texts'], data['train']['labels'])\n",
    "\n",
    "# Convert string labels to numeric\n",
    "if data['train']['labels'].dtype == object:  # Check if labels are strings\n",
    "    # Map 'Real' to 0 and 'Fake' to 1\n",
    "    data['train']['labels'] = data['train']['labels'].map({'Real': 0, 'Fake': 1})\n",
    "    data['test']['labels'] = data['test']['labels'].map({'Real': 0, 'Fake': 1})\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "logistic_model = LogisticModel(max_features=10000, preprocess=True)\n",
    "logistic_model.train(data['train']['texts'], data['train']['labels'])\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "logistic_metrics = logistic_model.evaluate(data['test']['texts'], data['test']['labels'])\n",
    "\n",
    "# Display metrics\n",
    "print(\"Logistic Regression Model Metrics:\")\n",
    "for metric, value in logistic_metrics.items():\n",
    "    if metric != 'confusion_matrix':\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    logistic_metrics['confusion_matrix'],\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Real', 'Fake'],\n",
    "    yticklabels=['Real', 'Fake']\n",
    ")\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "logistic_model.save('../models/logistic_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the DistilBERT model\n",
    "# Note: This may take some time to run\n",
    "distilbert_model = DistilBertModel(max_length=128, batch_size=16, epochs=2)\n",
    "distilbert_model.train(data['train']['texts'], data['train']['labels'])\n",
    "\n",
    "# Evaluate the model\n",
    "distilbert_metrics = distilbert_model.evaluate(data['test']['texts'], data['test']['labels'])\n",
    "\n",
    "# Display metrics\n",
    "print(\"DistilBERT Model Metrics:\")\n",
    "for metric, value in distilbert_metrics.items():\n",
    "    if metric != 'confusion_matrix':\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    distilbert_metrics['confusion_matrix'],\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Real', 'Fake'],\n",
    "    yticklabels=['Real', 'Fake']\n",
    ")\n",
    "plt.title('Confusion Matrix - DistilBERT')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "distilbert_model.save('../models/distilbert_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "models = ['Logistic Regression', 'DistilBERT']\n",
    "performance = {\n",
    "    'Logistic Regression': [logistic_metrics[metric] for metric in metrics],\n",
    "    'DistilBERT': [distilbert_metrics[metric] for metric in metrics]\n",
    "}\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "performance_df = pd.DataFrame(performance, index=metrics)\n",
    "performance_df.index = [metric.capitalize() for metric in metrics]\n",
    "\n",
    "# Display the comparison\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison\n",
    "performance_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Model')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some examples from the test set\n",
    "examples = data['test']['texts'].iloc[:5].tolist()\n",
    "true_labels = data['test']['labels'].iloc[:5].tolist()\n",
    "\n",
    "# Make predictions with both models\n",
    "logistic_preds = logistic_model.predict(examples)\n",
    "distilbert_preds = distilbert_model.predict(examples)\n",
    "\n",
    "# Display the results\n",
    "results = pd.DataFrame({\n",
    "    'Text': [text[:100] + '...' for text in examples],\n",
    "    'True Label': true_labels,\n",
    "    'Logistic Prediction': logistic_preds,\n",
    "    'DistilBERT Prediction': distilbert_preds\n",
    "})\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
