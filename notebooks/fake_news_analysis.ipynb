{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake News Analysis and Classification\n",
        "\n",
        "This notebook performs exploratory data analysis (EDA) on the fake news dataset and evaluates different classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add the parent directory to the path to import the models module\n",
        "sys.path.append('..')\n",
        "from models import load_data, LogisticModel, DistilBertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset from TSV files\n",
        "data = load_data('../data', use_predefined_splits=True)\n",
        "df = data['full_data']\n",
        "\n",
        "# Display basic information\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nOriginal label distribution:\\n{df['label'].value_counts()}\")\n",
        "print(f\"\\nOriginal label distribution (percentage):\\n{df['label'].value_counts(normalize=True) * 100}\")\n",
        "\n",
        "print(f\"\\nBinary label distribution:\\n{df['binary_label'].value_counts()}\")\n",
        "print(f\"\\nBinary label distribution (percentage):\\n{df['binary_label'].value_counts(normalize=True) * 100}\")\n",
        "\n",
        "# Display split sizes\n",
        "print(f\"\\nTrain set size: {len(data['train']['df'])}\")\n",
        "print(f\"Test set size: {len(data['test']['df'])}\")\n",
        "print(f\"Validation set size: {len(data['valid']['df'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis"
      ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "df['statement_length'] = df['statement'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='statement_length', hue='binary_label', bins=50, kde=True)\n",
    "plt.title('Statement Length Distribution by Class')\n",
    "plt.xlabel('Statement Length')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Real News (0)', 'Fake News (1)'])\n",
    "plt.show()"
   ]
  },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check for missing or null values\n",
    "print(\"Checking for missing values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "    \n",
    "# 2. Check label distribution\n",
    "print(\"\\nOriginal label distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\nUnique labels:\", df['label'].unique())\n",
    "\n",
    "print(\"\\nBinary label distribution:\")\n",
    "print(df['binary_label'].value_counts())\n",
    "\n",
    "# 3. Check for duplicates\n",
    "duplicate_count = df.duplicated(['statement', 'binary_label']).sum()\n",
    "print(f\"\\nNumber of duplicate entries (same statement and label): {duplicate_count}\")\n",
    "\n",
    "# Clean the data\n",
    "# Remove missing values if any\n",
    "df = df.dropna(subset=['statement', 'binary_label'])\n",
    "\n",
    "# Remove duplicates if any\n",
    "df = df.drop_duplicates(subset=['statement', 'binary_label'])\n",
    "\n",
    "# Show cleaned data stats\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(\"\\nBinary label distribution after cleaning:\")\n",
    "print(df['binary_label'].value_counts())\n",
    "\n",
    "# Update the data dictionary with cleaned data\n",
    "data['full_data'] = df\n"
   ]
  },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train and Evaluate Logistic Regression Model"
      ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "logistic_model = LogisticModel(max_features=10000, preprocess=True)\n",
    "logistic_model.train(data['train']['texts'], data['train']['labels'])\n",
    "\n",
    "# Evaluate the model on test set\n",
    "logistic_metrics = logistic_model.evaluate(data['test']['texts'], data['test']['labels'])\n",
    "\n",
    "# Display metrics\n",
    "print(\"Logistic Regression Model Metrics on Test Set:\")\n",
    "for metric, value in logistic_metrics.items():\n",
    "    if metric != 'confusion_matrix':\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    logistic_metrics['confusion_matrix'],\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Real', 'Fake'],\n",
    "    yticklabels=['Real', 'Fake']\n",
    ")\n",
    "plt.title('Confusion Matrix - Logistic Regression (Test Set)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on validation set\n",
    "logistic_val_metrics = logistic_model.evaluate(data['valid']['texts'], data['valid']['labels'])\n",
    "\n",
    "# Display validation metrics\n",
    "print(\"\\nLogistic Regression Model Metrics on Validation Set:\")\n",
    "for metric, value in logistic_val_metrics.items():\n",
    "    if metric != 'confusion_matrix':\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "logistic_model.save('../models/logistic_model.pkl')"
   ]
  },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train and Evaluate DistilBERT Model"
      ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the DistilBERT model\n",
    "# Note: This may take some time to run\n",
    "distilbert_model = DistilBertModel(max_length=128, batch_size=16, epochs=2)\n",
    "distilbert_model.train(data['train']['texts'], data['train']['labels'])\n",
    "\n",
    "# Evaluate the model on test set\n",
    "distilbert_metrics = distilbert_model.evaluate(data['test']['texts'], data['test']['labels'])\n",
    "\n",
    "# Display metrics\n",
    "print(\"DistilBERT Model Metrics on Test Set:\")\n",
    "for metric, value in distilbert_metrics.items():\n",
    "    if metric != 'confusion_matrix':\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    distilbert_metrics['confusion_matrix'],\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Real', 'Fake'],\n",
    "    yticklabels=['Real', 'Fake']\n",
    ")\n",
    "plt.title('Confusion Matrix - DistilBERT (Test Set)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on validation set\n",
    "distilbert_val_metrics = distilbert_model.evaluate(data['valid']['texts'], data['valid']['labels'])\n",
    "\n",
    "# Display validation metrics\n",
    "print(\"\\nDistilBERT Model Metrics on Validation Set:\")\n",
    "for metric, value in distilbert_val_metrics.items():\n",
    "    if metric != 'confusion_matrix':\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "distilbert_model.save('../models/distilbert_model')"
   ]
  },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare Model Performance"
      ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance on test set\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "models = ['Logistic Regression', 'DistilBERT']\n",
    "test_performance = {\n",
    "    'Logistic Regression': [logistic_metrics[metric] for metric in metrics],\n",
    "    'DistilBERT': [distilbert_metrics[metric] for metric in metrics]\n",
    "}\n",
    "\n",
    "# Create a DataFrame for test set comparison\n",
    "test_performance_df = pd.DataFrame(test_performance, index=metrics)\n",
    "test_performance_df.index = [metric.capitalize() for metric in metrics]\n",
    "\n",
    "# Display the test set comparison\n",
    "print(\"Model Performance Comparison on Test Set:\")\n",
    "test_performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance on validation set\n",
    "val_performance = {\n",
    "    'Logistic Regression': [logistic_val_metrics[metric] for metric in metrics],\n",
    "    'DistilBERT': [distilbert_val_metrics[metric] for metric in metrics]\n",
    "}\n",
    "\n",
    "# Create a DataFrame for validation set comparison\n",
    "val_performance_df = pd.DataFrame(val_performance, index=metrics)\n",
    "val_performance_df.index = [metric.capitalize() for metric in metrics]\n",
    "\n",
    "# Display the validation set comparison\n",
    "print(\"Model Performance Comparison on Validation Set:\")\n",
    "val_performance_df\n"
   ]
  },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test set comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "test_performance_df.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Model Performance Comparison - Test Set')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Model')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation set comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "val_performance_df.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Model Performance Comparison - Validation Set')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Model')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Combined plot for both test and validation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "test_performance_df.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Test Set Performance')\n",
    "axes[0].set_xlabel('Metric')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].legend(title='Model')\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "val_performance_df.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Validation Set Performance')\n",
    "axes[1].set_xlabel('Metric')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].legend(title='Model')\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Model Performance Comparison', fontsize=16, y=1.05)\n",
    "plt.show()"
   ]
  },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Example Predictions"
      ]
    },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some examples from the test set\n",
    "examples = data['test']['texts'].iloc[:5].tolist()\n",
    "true_labels = data['test']['labels'].iloc[:5].tolist()\n",
    "\n",
    "# Get additional metadata for context\n",
    "test_df = data['test']['df'].iloc[:5]\n",
    "speakers = test_df['speaker'].tolist()\n",
    "contexts = test_df['context'].tolist()\n",
    "original_labels = test_df['label'].tolist()\n",
    "\n",
    "# Make predictions with both models\n",
    "logistic_preds = logistic_model.predict(examples)\n",
    "distilbert_preds = distilbert_model.predict(examples)\n",
    "\n",
    "# Map binary labels to text for better readability\n",
    "label_text_map = {0: 'Real', 1: 'Fake'}\n",
    "true_labels_text = [label_text_map[label] for label in true_labels]\n",
    "logistic_preds_text = [label_text_map[pred] for pred in logistic_preds]\n",
    "distilbert_preds_text = [label_text_map[pred] for pred in distilbert_preds]\n",
    "\n",
    "# Display the results\n",
    "results = pd.DataFrame({\n",
    "    'Statement': [text[:100] + '...' for text in examples],\n",
    "    'Speaker': speakers,\n",
    "    'Context': contexts,\n",
    "    'Original Label': original_labels,\n",
    "    'Binary Label': true_labels_text,\n",
    "    'Logistic Prediction': logistic_preds_text,\n",
    "    'DistilBERT Prediction': distilbert_preds_text\n",
    "})\n",
    "\n",
    "results"
   ]
  }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "baml-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
