{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABBhulQB7D1P"
      },
      "source": [
        "# Fake News Analysis and Classification (Google Colab edition - TSV Dataset)",
        "",
        "This notebook performs exploratory data analysis (EDA) on the LIAR fake news dataset and evaluates different classification models. ",
        "This notebook is designed to run on Google Colab to leverage GPU acceleration for training our fake news detection models, particularly the DistilBERT transformer model which benefits significantly from GPU processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlknLwkZWlY_"
      },
      "outputs": [],
      "source": [
        "# Clone the repository",
        "!git clone https://github.com/smaoui-me/fake-news-predictor.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMHKT7747D1T"
      },
      "outputs": [],
      "source": [
        "import sys",
        "import os",
        "import pandas as pd",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "import torch",
        "",
        "sys.path.append('/content/fake-news-predictor')  # Path to the repo root",
        "",
        "from models import load_data, LogisticModel, DistilBertModel",
        "",
        "# Check if GPU is available",
        "print(f\"GPU available: {torch.cuda.is_available()}\")",
        "if torch.cuda.is_available():",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload the TSV Dataset Files",
        "",
        "You need to upload the train.tsv, test.tsv, and valid.tsv files to proceed with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files",
        "",
        "# Create data directory if it doesn't exist",
        "os.makedirs('/content/fake-news-predictor/data', exist_ok=True)",
        "",
        "# Upload the TSV files",
        "uploaded = files.upload()",
        "",
        "# Save the uploaded files to the data directory",
        "for filename in uploaded.keys():",
        "    if filename in ['train.tsv', 'test.tsv', 'valid.tsv']:",
        "        with open(f'/content/fake-news-predictor/data/{filename}', 'wb') as f:",
        "            f.write(uploaded[filename])",
        "        print(f'Saved {filename} to /content/fake-news-predictor/data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVPIWmAB7D1V"
      },
      "source": [
        "## 1. Load and Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GKbnPV07D1W"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from TSV files",
        "data = load_data('/content/fake-news-predictor/data', use_predefined_splits=True)",
        "df = data['full_data']",
        "",
        "# Display basic information",
        "print(f\"Dataset shape: {df.shape}\")",
        "print(f\"\nOriginal label distribution:\n{df['label'].value_counts()}\")",
        "print(f\"\nOriginal label distribution (percentage):\n{df['label'].value_counts(normalize=True) * 100}\")",
        "",
        "print(f\"\nBinary label distribution:\n{df['binary_label'].value_counts()}\")",
        "print(f\"\nBinary label distribution (percentage):\n{df['binary_label'].value_counts(normalize=True) * 100}\")",
        "",
        "# Display split sizes",
        "print(f\"\nTrain set size: {len(data['train']['df'])}\")",
        "print(f\"Test set size: {len(data['test']['df'])}\")",
        "print(f\"Validation set size: {len(data['valid']['df'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Display sample data",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "source": [
        "## 2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Text length distribution",
        "df['statement_length'] = df['statement'].apply(len)",
        "",
        "plt.figure(figsize=(12, 6))",
        "sns.histplot(data=df, x='statement_length', hue='binary_label', bins=50, kde=True)",
        "plt.title('Statement Length Distribution by Class')",
        "plt.xlabel('Statement Length')",
        "plt.ylabel('Count')",
        "plt.legend(['Real News (0)', 'Fake News (1)'])",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# 1. Check for missing or null values",
        "print(\"Checking for missing values:\")",
        "missing_values = df.isnull().sum()",
        "print(missing_values)",
        "    ",
        "# 2. Check label distribution",
        "print(\"\nOriginal label distribution:\")",
        "print(df['label'].value_counts())",
        "print(\"\nUnique labels:\", df['label'].unique())",
        "",
        "print(\"\nBinary label distribution:\")",
        "print(df['binary_label'].value_counts())",
        "",
        "# 3. Check for duplicates",
        "duplicate_count = df.duplicated(['statement', 'binary_label']).sum()",
        "print(f\"\nNumber of duplicate entries (same statement and label): {duplicate_count}\")",
        "",
        "# Clean the data",
        "# Remove missing values if any",
        "df = df.dropna(subset=['statement', 'binary_label'])",
        "",
        "# Remove duplicates if any",
        "df = df.drop_duplicates(subset=['statement', 'binary_label'])",
        "",
        "# Show cleaned data stats",
        "print(\"\nAfter cleaning:\")",
        "print(f\"Total samples: {len(df)}\")",
        "print(\"\nBinary label distribution after cleaning:\")",
        "print(df['binary_label'].value_counts())",
        "",
        "# Update the data dictionary with cleaned data",
        "data['full_data'] = df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "source": [
        "## 3. Train and Evaluate Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the Logistic Regression model",
        "logistic_model = LogisticModel(max_features=10000, preprocess=True)",
        "logistic_model.train(data['train']['texts'], data['train']['labels'])",
        "",
        "# Evaluate the model on test set",
        "logistic_metrics = logistic_model.evaluate(data['test']['texts'], data['test']['labels'])",
        "",
        "# Display metrics",
        "print(\"Logistic Regression Model Metrics on Test Set:\")",
        "for metric, value in logistic_metrics.items():",
        "    if metric != 'confusion_matrix':",
        "        print(f\"{metric.capitalize()}: {value:.4f}\")",
        "",
        "# Plot confusion matrix",
        "plt.figure(figsize=(8, 6))",
        "sns.heatmap(",
        "    logistic_metrics['confusion_matrix'],",
        "    annot=True,",
        "    fmt='d',",
        "    cmap='Blues',",
        "    xticklabels=['Real', 'Fake'],",
        "    yticklabels=['Real', 'Fake']",
        ")",
        "plt.title('Confusion Matrix - Logistic Regression (Test Set)')",
        "plt.xlabel('Predicted')",
        "plt.ylabel('Actual')",
        "plt.show()",
        "",
        "# Evaluate on validation set",
        "logistic_val_metrics = logistic_model.evaluate(data['valid']['texts'], data['valid']['labels'])",
        "",
        "# Display validation metrics",
        "print(\"\nLogistic Regression Model Metrics on Validation Set:\")",
        "for metric, value in logistic_val_metrics.items():",
        "    if metric != 'confusion_matrix':",
        "        print(f\"{metric.capitalize()}: {value:.4f}\")",
        "",
        "# Save the model",
        "os.makedirs('/content/models', exist_ok=True)",
        "logistic_model.save('/content/models/logistic_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "source": [
        "## 4. Train and Evaluate DistilBERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the DistilBERT model",
        "# Note: This may take some time to run",
        "distilbert_model = DistilBertModel(max_length=128, batch_size=16, epochs=2)",
        "distilbert_model.train(data['train']['texts'], data['train']['labels'])",
        "",
        "# Evaluate the model on test set",
        "distilbert_metrics = distilbert_model.evaluate(data['test']['texts'], data['test']['labels'])",
        "",
        "# Display metrics",
        "print(\"DistilBERT Model Metrics on Test Set:\")",
        "for metric, value in distilbert_metrics.items():",
        "    if metric != 'confusion_matrix':",
        "        print(f\"{metric.capitalize()}: {value:.4f}\")",
        "",
        "# Plot confusion matrix",
        "plt.figure(figsize=(8, 6))",
        "sns.heatmap(",
        "    distilbert_metrics['confusion_matrix'],",
        "    annot=True,",
        "    fmt='d',",
        "    cmap='Blues',",
        "    xticklabels=['Real', 'Fake'],",
        "    yticklabels=['Real', 'Fake']",
        ")",
        "plt.title('Confusion Matrix - DistilBERT (Test Set)')",
        "plt.xlabel('Predicted')",
        "plt.ylabel('Actual')",
        "plt.show()",
        "",
        "# Evaluate on validation set",
        "distilbert_val_metrics = distilbert_model.evaluate(data['valid']['texts'], data['valid']['labels'])",
        "",
        "# Display validation metrics",
        "print(\"\nDistilBERT Model Metrics on Validation Set:\")",
        "for metric, value in distilbert_val_metrics.items():",
        "    if metric != 'confusion_matrix':",
        "        print(f\"{metric.capitalize()}: {value:.4f}\")",
        "",
        "# Save the model",
        "distilbert_model.save('/content/models/distilbert_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "source": [
        "## 5. Compare Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Compare model performance on test set",
        "metrics = ['accuracy', 'precision', 'recall', 'f1_score']",
        "models = ['Logistic Regression', 'DistilBERT']",
        "test_performance = {",
        "    'Logistic Regression': [logistic_metrics[metric] for metric in metrics],",
        "    'DistilBERT': [distilbert_metrics[metric] for metric in metrics]",
        "}",
        "",
        "# Create a DataFrame for test set comparison",
        "test_performance_df = pd.DataFrame(test_performance, index=metrics)",
        "test_performance_df.index = [metric.capitalize() for metric in metrics]",
        "",
        "# Display the test set comparison",
        "print(\"Model Performance Comparison on Test Set:\")",
        "test_performance_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Compare model performance on validation set",
        "val_performance = {",
        "    'Logistic Regression': [logistic_val_metrics[metric] for metric in metrics],",
        "    'DistilBERT': [distilbert_val_metrics[metric] for metric in metrics]",
        "}",
        "",
        "# Create a DataFrame for validation set comparison",
        "val_performance_df = pd.DataFrame(val_performance, index=metrics)",
        "val_performance_df.index = [metric.capitalize() for metric in metrics]",
        "",
        "# Display the validation set comparison",
        "print(\"Model Performance Comparison on Validation Set:\")",
        "val_performance_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Plot the test set comparison",
        "plt.figure(figsize=(12, 6))",
        "test_performance_df.plot(kind='bar', ax=plt.gca())",
        "plt.title('Model Performance Comparison - Test Set')",
        "plt.xlabel('Metric')",
        "plt.ylabel('Score')",
        "plt.ylim(0, 1)",
        "plt.legend(title='Model')",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)",
        "plt.show()",
        "",
        "# Plot the validation set comparison",
        "plt.figure(figsize=(12, 6))",
        "val_performance_df.plot(kind='bar', ax=plt.gca())",
        "plt.title('Model Performance Comparison - Validation Set')",
        "plt.xlabel('Metric')",
        "plt.ylabel('Score')",
        "plt.ylim(0, 1)",
        "plt.legend(title='Model')",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)",
        "plt.show()",
        "",
        "# Combined plot for both test and validation",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))",
        "",
        "test_performance_df.plot(kind='bar', ax=axes[0])",
        "axes[0].set_title('Test Set Performance')",
        "axes[0].set_xlabel('Metric')",
        "axes[0].set_ylabel('Score')",
        "axes[0].set_ylim(0, 1)",
        "axes[0].legend(title='Model')",
        "axes[0].grid(axis='y', linestyle='--', alpha=0.7)",
        "",
        "val_performance_df.plot(kind='bar', ax=axes[1])",
        "axes[1].set_title('Validation Set Performance')",
        "axes[1].set_xlabel('Metric')",
        "axes[1].set_ylabel('Score')",
        "axes[1].set_ylim(0, 1)",
        "axes[1].legend(title='Model')",
        "axes[1].grid(axis='y', linestyle='--', alpha=0.7)",
        "",
        "plt.tight_layout()",
        "plt.suptitle('Model Performance Comparison', fontsize=16, y=1.05)",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "source": [
        "## 6. Example Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Get some examples from the test set",
        "examples = data['test']['texts'].iloc[:5].tolist()",
        "true_labels = data['test']['labels'].iloc[:5].tolist()",
        "",
        "# Get additional metadata for context",
        "test_df = data['test']['df'].iloc[:5]",
        "speakers = test_df['speaker'].tolist()",
        "contexts = test_df['context'].tolist()",
        "original_labels = test_df['label'].tolist()",
        "",
        "# Make predictions with both models",
        "logistic_preds = logistic_model.predict(examples)",
        "distilbert_preds = distilbert_model.predict(examples)",
        "",
        "# Map binary labels to text for better readability",
        "label_text_map = {0: 'Real', 1: 'Fake'}",
        "true_labels_text = [label_text_map[label] for label in true_labels]",
        "logistic_preds_text = [label_text_map[pred] for pred in logistic_preds]",
        "distilbert_preds_text = [label_text_map[pred] for pred in distilbert_preds]",
        "",
        "# Display the results",
        "results = pd.DataFrame({",
        "    'Statement': [text[:100] + '...' for text in examples],",
        "    'Speaker': speakers,",
        "    'Context': contexts,",
        "    'Original Label': original_labels,",
        "    'Binary Label': true_labels_text,",
        "    'Logistic Prediction': logistic_preds_text,",
        "    'DistilBERT Prediction': distilbert_preds_text",
        "})",
        "",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "source": [
        "## 7. Download Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql-Ys-Oi7D1X"
      },
      "outputs": [],
      "source": [
        "# Download the trained models",
        "from google.colab import files",
        "",
        "# Compress the models directory",
        "!zip -r /content/models.zip /content/models",
        "",
        "# Download the compressed file",
        "files.download('/content/models.zip')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}